\relax 
\citation{Raibert2008}
\citation{guizzo2011google}
\citation{Krizhevsky2012}
\citation{Hinton2006}
\citation{Salakhutdinov2009}
\citation{Krizhevsky2012}
\citation{Vincent2010}
\citation{OConnor2013}
\citation{Cho2014}
\citation{Lenz2013}
\citation{Hoffman2014}
\citation{Mayer2006}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\citation{Graves2013}
\citation{Graves2013b}
\citation{Graves2014}
\citation{Sutton1998a}
\citation{williams1989learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Reinforcement Learning}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov Decision Process}{3}}
\citation{Kober2013}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Partially Observable Markov Decision Process}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Markov Decision Process with Continuous States}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Value Functions}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Natural Actor Critic Model}{7}}
\newlabel{objective_function}{{2.5}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Reinforcement Learning Methods}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Policy Evaluation}{8}}
\citation{peters2006policy}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Tabular TD(0) policy evaluation algorithm}}{9}}
\newlabel{algo:adam}{{1}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Policy Gradient Methods}{9}}
\citation{Kober2013}
\citation{peters2006policy}
\citation{Kober2013}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Classification of the Regarded RL Problems}{10}}
\citation{Kober2013}
\citation{sehnke2008policy}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Policy Gradient with Parameter Exploration}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}PGPE algorithm}{11}}
\newlabel{pgpe_gaussian}{{2.15}{12}}
\newlabel{stadard_identity_transformation}{{2.17}{12}}
\newlabel{markov_property}{{2.18}{12}}
\newlabel{log_markov_property}{{2.19}{13}}
\newlabel{expansion_of_log_history}{{2.22}{13}}
\newlabel{standard_identity_transformation}{{2.23}{13}}
\newlabel{pgpe_policy}{{2.24}{13}}
\newlabel{standard_identity_transformation_replaced_by_mean_sigma}{{{2.25}}{13}}
\citation{williams1992simple}
\newlabel{gaussian_mean_gradient}{{{2.26}}{14}}
\newlabel{gaussian_sigma_gradient}{{{2.27}}{14}}
\newlabel{wiliams_update_mean}{{2.28}{14}}
\newlabel{wiliams_update_sigma}{{2.29}{14}}
\citation{sehnke2013efficient}
\citation{zhao2011analysis}
\newlabel{sys_update_mu}{{2.31}{15}}
\newlabel{sys_update_sigma}{{2.32}{15}}
\newlabel{b_update}{{2.33}{15}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Policy Gradient with Parameter Exploration}}{15}}
\newlabel{alg:pepg}{{1}{15}}
\citation{hinton2006reducing}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Deep Recurrent Neural Networks}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Deep Learning and its Recent Advances}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Feedforward Neural Networks}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces This picture shows the structure of perceptron, the i'th input is shown as $x_i$ and corresponded weights are denoted as $w_i$. There is also a bias term name $b$ connected to cell, which, with a addition function, generates the output.}}{18}}
\newlabel{proceptron}{{3.1}{18}}
\newlabel{pronceptron_formula}{{3.2}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces This picture shows the structure of Feed-forward Neural Network. There are three layers in this network, namely the input layer, hidden layer and the output layer. For connections from input layer to hidden layer, the i'th input is shown as $x_i$ and corresponded weights of hidden neuron $j$ is denoted as $W_{ij}$.}}{19}}
\newlabel{feedforward_nn}{{3.2}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Recurrent Neural Networks}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces This picture shows the structure of recurrent neural network. There are two layers in this networ namely input layer and hidden layer, the first layer contains notes connected to the input data. The second layer stores information and also forwards information to next layer. The cell in the hidden layer has a connection to itself which means the information stored in the cell at time $t-1$ also influences the information stored in the cell at time $t$}}{20}}
\newlabel{feedforward_nn}{{3.3}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Finite Unfolding in Time}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces This picture shows the structure of a simple RNN. It contains three layers i.e. one input layer, one hidden layer and one output layer.}}{21}}
\newlabel{simple_rnn}{{3.4}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces This picture contains model that unfolds a simple RNN described in Figure3.4\hbox {} in abitary $n$ time steps. In this figure, $x_t$ represents input layer at time $t$, $s_t$ representes the hiddent states at time $t$ and $o_t$ representes output layer at time $t$.}}{21}}
\newlabel{unfolded_simple_rnn}{{3.5}{21}}
\newlabel{internal_state_fomula}{{3.4}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Overshooting}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces This figure isllustrates in a predictive model made by RNN, how does it generate $\mathaccentV {hat}05E{x_t}$ based on previous data. After using all training example, the model lacks input data. Then the neural caluclation of hidden defined as $\mathaccentV {vec}17E{s_{t}} = \sigma (W\mathaccentV {vec}17E{x_t} + B\mathaccentV {vec}17E{s_{t-1}}))$ becomes $\mathaccentV {vec}17E{s_{t}} = \sigma (B\mathaccentV {vec}17E{s_{t-1}})$}}{22}}
\newlabel{overshot_rnn}{{3.6}{22}}
\newlabel{overshot_formula}{{3.5}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Dynamical Consistency}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces This picture contains model for predicting data for time series. For the first unfolded structure of neural network, the data of series at time t-2 is fed to the network and we expect data at time t-1 are predict by the network.}}{23}}
\newlabel{series_prediction_rnn}{{3.7}{23}}
\newlabel{training_rnn}{{3.8}{23}}
\newlabel{prediction_rnn}{{3.9}{23}}
\newlabel{train_output}{{3.10}{23}}
\newlabel{train_prediction}{{3.11}{23}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Universal Approximation}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Training of RNN}{24}}
\newlabel{training_rnn}{{3.5}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Shared Weight Extended Backpropagation}{25}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Backpropagation for two layers feed-forward neural network.}}{25}}
\newlabel{backpropagation}{{3}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Learning Long-Term Dependencies}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces This figure shows the basic strucuture of LSTM. In the middel, there is the memory cell which keeps the information of the data sequence. Around it, there are three gates, namely input gate, output gate and foget gate. Each of them gets information from the input and controls the updatting rule of memory cell. At the left side, there is input connection. On right side, there is output connection of this layer.}}{26}}
\newlabel{series_prediction_rnn}{{3.8}{26}}
\newlabel{eq_netout}{{3.17}{27}}
\newlabel{eq_netin}{{3.18}{27}}
\newlabel{eq_cell}{{3.19}{27}}
\newlabel{eq_cell_out}{{3.20}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Optimization Methods}{28}}
\citation{kingma2014adam}
\@writefile{toc}{\contentsline {subsubsection}{Nesterovs Accelerated Gradient}{29}}
\newlabel{normal_gradient_descent}{{3.22}{29}}
\newlabel{class_momentum}{{3.23}{29}}
\newlabel{nag}{{3.24}{29}}
\@writefile{toc}{\contentsline {subsubsection}{Adam}{29}}
\@writefile{loa}{\contentsline {algocf}{\numberline {4}{\ignorespaces \emph  {Adam}, $g^2_t$ indicates the elementwise square $g_t \odot g_t$. Good default settings for the tested machine learning problems are $\alpha =0.001$, $\beta _1=0.9$, $\beta _2=0.999$, $\epsilon = 10^{-8}$ and $\lambda = 1-10^{-8}$.}}{30}}
\newlabel{algo:adam}{{4}{30}}
\citation{thrun1996explanation}
\citation{nguyen1990neural}
\citation{mnih2013playing}
\citation{mnih2015human}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Prior Arts of Combining Deep Neuron Network and RL}{31}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Deep Q Network}{31}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiment}{33}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Cart-pole Balancing}{33}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces This figure shows the cart-pole balancing system. A force $\mathaccentV {vec}17E{F}$ is exerted on the cart C with mass $M$ to balance a pole l that also has length $l$ attached to it. A mass $m$ is attached on pole L. The system has four states including point of the system $s$, velocity of the system $\mathaccentV {dot}05F{s}$, angel between pole L and normal $\theta $ and angular acceleration $ \mathaccentV {dot}05F{\theta }$. When force $\mathaccentV {vec}17E{F}$ is exerted on cart C, the L will also have a force on the point of attachment with cart and the task is to provide forces that can keep position of cart as well as angel $\theta $ between pole and normal in a range of allowed values(e.g. $s \in [-4,4]$, $\theta \in [-0.2, 0.2]$)}}{34}}
\newlabel{cart_pendulum}{{5.1}{34}}
\citation{pybrain2010jmlr}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}System Implementation}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Experiment Result}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces This figure shows the result of benchmark of cart-pole balancing problem. The x-axis represents the real word samples needed for training and the y-axis represents the average reward of 50 trials.}}{36}}
\newlabel{cart_pole_pgpe_benchmark}{{5.2}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Baxter Robot Learning a Action}{37}}
\newlabel{baxter}{{5.2}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces This figure shows a baxter research robot. We can see that it has a face and two arms.}}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}System Implementation}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Experiment Result Using Baxter Simulator}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Experiment Result Using Baxter Robot}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces This picture shows the Baxter robot and its corresponding joints. Seven degrees of freedom including shoulder roll $s_0$, elbow roll $e_0$, wrist roll $w_0$, wrist roll $w_2$, shoulder pitch $s_1$ , elbow pitch $e_1$ and wrist pitch $w_1$ are presented on picture.}}{39}}
\bibdata{library}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Future Research}{41}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Combining Control with Vision Using Deep Neural Network}{41}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Transfer Learning for Learning Repetitive Behaviour Using Deep Neural Network}{41}}
\bibcite{Cho2014}{1}
\bibcite{Graves2013}{2}
\bibcite{Graves2014}{3}
\bibcite{guizzo2011google}{4}
\bibcite{Hinton2006}{5}
\bibcite{hinton2006reducing}{6}
\bibcite{Hoffman2014}{7}
\bibcite{kingma2014adam}{8}
\bibcite{Kober2013}{9}
\bibcite{Krizhevsky2012}{10}
\bibcite{Lenz2013}{11}
\@writefile{toc}{\contentsline {chapter}{References}{43}}
\bibcite{Mayer2006}{12}
\bibcite{mnih2013playing}{13}
\bibcite{mnih2015human}{14}
\bibcite{nguyen1990neural}{15}
\bibcite{OConnor2013}{16}
\bibcite{peters2006policy}{17}
\bibcite{Raibert2008}{18}
\bibcite{Salakhutdinov2009}{19}
\bibcite{pybrain2010jmlr}{20}
\bibcite{sehnke2013efficient}{21}
\bibcite{sehnke2008policy}{22}
\bibcite{thrun1996explanation}{23}
\bibcite{Vincent2010}{24}
\bibcite{williams1992simple}{25}
\bibcite{zhao2011analysis}{26}
\bibstyle{siam}
