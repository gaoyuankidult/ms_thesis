\documentclass[officiallayout]{tktla} 
%\documentclass[officiallayout,a4frame]{tktla}
\usepackage[latin1]{inputenc}
\usepackage{latexsym}
\usepackage{graphicx}

\title{Deep Learning Algorithms \\ for Control}
\author{Yuan Gao}
\authorcontact{gaoyuankidult@gmail.com\par
  http://www.cs.helsinki.fi/u/yuangao/}
\pubtime{September}{2015}
\reportno{0}
\isbnpaperback{000-00-0000-0}
\isbnpdf{000-00-0000-0}
\issn{1238-8645}
\printhouse{Unigrafia}
\pubpages{7} % --- remember to update this!
% For monographs, the number of the last page of the list of references
% For article-based theses, the number of the last page of the list of
% references of the preamble part + the total number of the pages of
% the original articles and interleaf pages.
\supervisorlist{Dorota Glowacka, University of Helsinki, Finland \newline  Leo K{\"a}rkk{\"a}inen, Nokia Research Center, Finland \newline Honkala Mikko Nokia Research Center}
\preexaminera{}
\preexaminerb{}
\opponent{}
\custos{}
\generalterms{}
\additionalkeywords{}
\crcshort{A.0, C.0.0}
\crclong{
\item[A.0] Example Category
\item[C.0.0] Another Example
}
\permissionnotice{
  To be presented in \ldots{} text of a long permission notice. Text of
  a long permission notice. Text of a long permission notice. Text of
  a long permission notice. Text of a long permission notice. Text of
  a long permission notice.
}

\newtheorem{theorem}{Theorem}[chapter]
\newenvironment{proof}{\noindent\textbf{Proof.} }{$\Box$}

\begin{document}

\frontmatter

\maketitle

\begin{abstract}
One sub-field of machine learning called deep learning gained a lot of attention recently as a method attempting to model high-level abstractions by using model architectures composed by multiple non-linear layers. (for example \cite{Krizhevsky2012}). Several architectures of deep learning networks like deep belief network \cite{Hinton2006}, deep Boltzman machine \cite{Salakhutdinov2009}, convolutional neural network \cite{Krizhevsky2012} and deep de-noising auto-encoder \cite{Vincent2010} have shown its advantages in specific areas. One interesting example is convolutional neural network invented by Krizhevsky in 2012, which  outperformed all the traditional feature-based machine learning techniques in ImageNet competition.
\end{abstract}

\begin{acknowledgements}
  This is a sample sentence that should look like normal text, and
  this is another. This is a sample sentence that should look like
  normal text, and this is another. This is a sample sentence that
  should look like normal text, and this is another.
\end{acknowledgements}

\tableofcontents

\mainmatter

\chapter{Reinforcement Learning}
\section{Markov Decision Process}
\section{Partially Observable Markov Decision Process}
\section{Dynamic Programming}
\section{Reinforcement Learning Methods}
\subsection{Temporal Difference Learning}
\subsection{Q-Learning}
\subsection{Adaptive Heuristic Critic}
\subsection{Prioritised Sweeping}
\subsection{Policy Gradient Methods}

\section{Classification of the Regarded RL Problems}
\subsection{High-Dimensionality}
\subsection{Partial-Observability}
\subsection{Continuous State and Action Spaces}
\subsection{Data-Efficiency}

\chapter{Recurrent Neural Networks}
\section{Feedforward Neural Networks}
\section{Recurrent Neural Networks}
\subsection{Finite Unfolding in Time}
\subsection{Overshooting}
\subsection{Dynamical Consistency}
\section{Universal Approximation}
\subsection{Approximation by FFNN}

\subsection{Approximation by RNN}
\section{Training of RNN}
\subsection{Shared Weight Extended Backpropagation}
\subsection{Learning Methods}
\subsection{Learning Long-Term Dependencies}

\section{Improved Model-Building with RNN} 
\subsection{Handling Data Noise} 
\subsection{Handling the Uncertainty of the Initial State}
\subsection{Optimal Weight Initialisation}

\chapter{Prior Arts of Combining RNN and RL}
\section{Neural Actor-Critic(idasi's group)}
\section{LSTM with POMDP objective function}
\section{PhD thesis, by Remi Coulom ?}

\section{DQN?}
\section{Hybrid Approch(RL with RNN)}
\section{Recurrent Models of Visual Attention?}       
\section{stanley gecco021 2002?}
\chapter{Experiment}
\section{RNN(LSTM) Implementation}
\section{Cart-pole Balancing Simulator}
\section{Learning a task of stacking wooden blocks}

This is a sample sentence that should look like normal text, and this
is another. This is a sample sentence that should look like normal
text, and this is another. This is a sample sentence that should look
like normal text, and this is another.


\begin{theorem}
This is a sample sentence that should look like normal text,
and this is another:
\[ y = x+3 \]
\end{theorem}

\begin{proof}
This is a sample sentence.
\end{proof}

\bibliography{library}
\bibliographystyle{alpha}

\end{document}
